{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stage2nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q228h_LE2X4w"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler    \r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76JbgNqUhAU9",
        "outputId": "f62b7ed7-21eb-4930-be0c-c58190e5079b"
      },
      "source": [
        "from google.colab import drive \r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "df = pd.read_csv('gdrive/My Drive/pred_all_models_train.csv').drop(columns = 'Unnamed: 0')\r\n",
        "\r\n",
        "X = df.iloc[:, 1:].values\r\n",
        "\r\n",
        "textRep = True\r\n",
        "if textRep:\r\n",
        "  embedding = 'skipgram' # or cbow\r\n",
        "  X1_0 = np.load('gdrive/My Drive/'+embedding + '_train_neg_full_u.npy')\r\n",
        "  X1_1 = np.load('gdrive/My Drive/'+embedding + '_train_pos_full_u.npy')\r\n",
        "  X1 = np.concatenate((X1_0, X1_1))\r\n",
        "  X = np.concatenate((X, X1), axis = 1)\r\n",
        "\r\n",
        "y = df.iloc[:, 0].values\r\n",
        "\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=0)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_val = scaler.fit_transform(X_val)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTYiARb5Amf"
      },
      "source": [
        "class trainData(Dataset):\r\n",
        "    def __init__(self, X_data, y_data):\r\n",
        "        self.X_data = X_data\r\n",
        "        self.y_data = y_data\r\n",
        "        \r\n",
        "    def __getitem__(self, index):\r\n",
        "        return self.X_data[index], self.y_data[index]\r\n",
        "        \r\n",
        "    def __len__ (self):\r\n",
        "        return len(self.X_data)\r\n",
        "\r\n",
        "xdim = X.shape[1]\r\n",
        "del df, X, y\r\n",
        "train_data = trainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\r\n",
        "val_data = trainData(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\r\n",
        "del X_train, X_val, y_train, y_val\r\n",
        "BATCH_SIZE = 64\r\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\r\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, drop_last = True)  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WrffIJY6F-M"
      },
      "source": [
        "class stage2clf(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(stage2clf, self).__init__()\r\n",
        "        self.hidden_layer_1 = nn.Linear(xdim, 64) \r\n",
        "        self.hidden_layer_2 = nn.Linear(64, 64)\r\n",
        "        self.output_layer = nn.Linear(64, 1) \r\n",
        "        \r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.drop = nn.Dropout(p=0.1)\r\n",
        "        self.batch_norm = nn.BatchNorm1d(64)\r\n",
        "        \r\n",
        "    def forward(self, inputs):\r\n",
        "        x = inputs\r\n",
        "        x = self.relu(self.hidden_layer_1(x))\r\n",
        "        x = self.batch_norm(x)\r\n",
        "        x = self.relu(self.hidden_layer_2(x))\r\n",
        "        x = self.batch_norm(x)\r\n",
        "        x = self.drop(x)\r\n",
        "        x = self.output_layer(x)\r\n",
        "        \r\n",
        "        return x\r\n",
        "\r\n",
        "class stage2clf2(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(stage2clf2, self).__init__()\r\n",
        "        self.hidden_layer_1 = nn.Linear(xdim, 128) \r\n",
        "        self.hidden_layer_2 = nn.Linear(128, 64)\r\n",
        "        self.output_layer = nn.Linear(64, 1) \r\n",
        "        \r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.drop = nn.Dropout(p=0.1)\r\n",
        "        self.batch_norm1 = nn.BatchNorm1d(128)\r\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\r\n",
        "        \r\n",
        "    def forward(self, inputs):\r\n",
        "        x = inputs\r\n",
        "        x = self.relu(self.hidden_layer_1(x))\r\n",
        "        x = self.batch_norm1(x)\r\n",
        "        x = self.relu(self.hidden_layer_2(x))\r\n",
        "        x = self.batch_norm2(x)\r\n",
        "        x = self.drop(x)\r\n",
        "        x = self.output_layer(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2ALqTOf6Ngo"
      },
      "source": [
        "model = stage2clf2()\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "LEARNING_RATE = 0.001\r\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\r\n",
        "\r\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSTZmHqo6UVv"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\r\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\r\n",
        "\r\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\r\n",
        "    acc = correct_results_sum/y_test.shape[0]\r\n",
        "    acc = torch.round(acc * 100)\r\n",
        "    return acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIjnd-RUnYN-"
      },
      "source": [
        "def validation_stats(network, loader):\r\n",
        "\r\n",
        "    acc = []\r\n",
        "    with torch.no_grad():\r\n",
        "        for x, y in loader:\r\n",
        "          x, y = x.to(device), y.to(device)\r\n",
        "          y_pred = network(x)\r\n",
        "          acc.append(binary_acc(y_pred, y.unsqueeze(1)))\r\n",
        "\r\n",
        "    acc = torch.Tensor(acc)\r\n",
        "    return acc.mean()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDFkpfbL6WV7",
        "outputId": "08351789-43db-4e3e-d83c-70e6ef528c87"
      },
      "source": [
        "EPOCHS = 10\r\n",
        "\r\n",
        "\r\n",
        "model.train()\r\n",
        "for e in range(1, EPOCHS+1):\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    for X_batch, y_batch in train_loader:\r\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        y_pred = model(X_batch)\r\n",
        "        \r\n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\r\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc.item()\r\n",
        "    val_acc = validation_stats(model, val_loader)\r\n",
        "    print('Epoch {:d}.\\tLoss: {:.5f}\\tAccuracy: {:.3f}% (train) / {:.3f}% (val)'.format(e, 100*epoch_loss/len(train_loader), epoch_acc/len(train_loader), val_acc))\r\n",
        "    scheduler.step()\r\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1.\tLoss: 21.64399\tAccuracy: 91.372% (train) / 91.531% (val)\n",
            "Epoch 2.\tLoss: 21.01288\tAccuracy: 91.588% (train) / 91.573% (val)\n",
            "Epoch 3.\tLoss: 20.91626\tAccuracy: 91.612% (train) / 91.627% (val)\n",
            "Epoch 4.\tLoss: 20.85550\tAccuracy: 91.626% (train) / 91.637% (val)\n",
            "Epoch 5.\tLoss: 20.82002\tAccuracy: 91.644% (train) / 91.621% (val)\n",
            "Epoch 6.\tLoss: 20.77931\tAccuracy: 91.663% (train) / 91.596% (val)\n",
            "Epoch 7.\tLoss: 20.76094\tAccuracy: 91.668% (train) / 91.637% (val)\n",
            "Epoch 8.\tLoss: 20.72778\tAccuracy: 91.667% (train) / 91.661% (val)\n",
            "Epoch 9.\tLoss: 20.70417\tAccuracy: 91.696% (train) / 91.656% (val)\n",
            "Epoch 10.\tLoss: 20.68176\tAccuracy: 91.691% (train) / 91.622% (val)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0DJgCUr8PDf"
      },
      "source": [
        "X_t = pd.read_csv('gdrive/My Drive/pred_all_models_test.csv').drop(columns = 'Unnamed: 0')\r\n",
        "\r\n",
        "if textRep:\r\n",
        "  X1 = np.load('gdrive/My Drive/'+ embedding + '_test.npy')\r\n",
        "  X_t = np.concatenate((X_t, X1), axis = 1)\r\n",
        "X_t = scaler.transform(X_t)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrZZuyP8_KAo"
      },
      "source": [
        "class testData(Dataset):\r\n",
        "    def __init__(self, X_data):\r\n",
        "        self.X_data = X_data\r\n",
        "        \r\n",
        "    def __getitem__(self, index):\r\n",
        "        return self.X_data[index]\r\n",
        "        \r\n",
        "    def __len__ (self):\r\n",
        "        return len(self.X_data)\r\n",
        "    \r\n",
        "\r\n",
        "test_data = testData(torch.FloatTensor(X_t))\r\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=10, shuffle=False, drop_last=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIqd6qG1_o7G"
      },
      "source": [
        "y_pred_list = []\r\n",
        "model.eval()\r\n",
        "with torch.no_grad():\r\n",
        "    for X_batch in test_loader:\r\n",
        "        X_batch = X_batch.to(device)\r\n",
        "        pred = torch.round(torch.sigmoid(model(X_batch)))\r\n",
        "        y_pred_list.append(pred.cpu().numpy())\r\n",
        "\r\n",
        "y_pred = [a.squeeze().tolist() for a in y_pred_list]\r\n",
        "predictions = []\r\n",
        "for row in y_pred:\r\n",
        "  predictions += row\r\n",
        "predictions = np.array(predictions)\r\n",
        "preds = pd.DataFrame((2*predictions-1).astype(int), columns = ['Prediction'], index = np.arange(1, len(predictions)+1))\r\n",
        "preds.index.names = ['Id']\r\n",
        "preds.to_csv('stage2_nn.csv')"
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}