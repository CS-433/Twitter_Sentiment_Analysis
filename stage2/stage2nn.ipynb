{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stage2nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q228h_LE2X4w"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler    \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76JbgNqUhAU9",
        "outputId": "dc2d82f4-faf9-4cb0-f421-eac080ace2d0"
      },
      "source": [
        "from google.colab import drive \r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "df = pd.read_csv('gdrive/My Drive/pred_all_models_train.csv').drop(columns = 'Unnamed: 0')\r\n",
        "\r\n",
        "X = df.iloc[:, 1:].values\r\n",
        "\r\n",
        "textRep = False\r\n",
        "if textRep:\r\n",
        "  model = 'skipgram' # or cbow\r\n",
        "  X1_0 = np.load('gdrive/My Drive/'+model + '_train_neg_full_u.npy')\r\n",
        "  X1_1 = np.load('gdrive/My Drive/'+model + '_train_pos_full_u.npy')\r\n",
        "  X1 = np.concatenate((X1_0, X1_1))\r\n",
        "  X = np.concatenate((X, X1), axis = 1)\r\n",
        "\r\n",
        "y = df.iloc[:, 0].values\r\n",
        "\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=0)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_val = scaler.fit_transform(X_val)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTYiARb5Amf"
      },
      "source": [
        "class trainData(Dataset):\r\n",
        "    def __init__(self, X_data, y_data):\r\n",
        "        self.X_data = X_data\r\n",
        "        self.y_data = y_data\r\n",
        "        \r\n",
        "    def __getitem__(self, index):\r\n",
        "        return self.X_data[index], self.y_data[index]\r\n",
        "        \r\n",
        "    def __len__ (self):\r\n",
        "        return len(self.X_data)\r\n",
        "\r\n",
        "\r\n",
        "train_data = trainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\r\n",
        "val_data = trainData(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\r\n",
        "BATCH_SIZE = 64\r\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\r\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=64)  "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WrffIJY6F-M"
      },
      "source": [
        "class stage2clf(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(stage2clf, self).__init__()\r\n",
        "        self.hidden_layer_1 = nn.Linear(X.shape[1], 64) \r\n",
        "        self.hidden_layer_2 = nn.Linear(64, 64)\r\n",
        "        self.output_layer = nn.Linear(64, 1) \r\n",
        "        \r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.drop = nn.Dropout(p=0.1)\r\n",
        "        self.batch_norm = nn.BatchNorm1d(64)\r\n",
        "        \r\n",
        "    def forward(self, inputs):\r\n",
        "        x = inputs\r\n",
        "        x = self.relu(self.hidden_layer_1(x))\r\n",
        "        x = self.batch_norm(x)\r\n",
        "        x = self.relu(self.hidden_layer_2(x))\r\n",
        "        x = self.batch_norm(x)\r\n",
        "        x = self.drop(x)\r\n",
        "        x = self.output_layer(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2ALqTOf6Ngo"
      },
      "source": [
        "model = stage2clf()\r\n",
        "model.to(device)\r\n",
        "\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "LEARNING_RATE = 0.001\r\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\r\n",
        "\r\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSTZmHqo6UVv"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\r\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\r\n",
        "\r\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\r\n",
        "    acc = correct_results_sum/y_test.shape[0]\r\n",
        "    acc = torch.round(acc * 100)\r\n",
        "    return acc"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIjnd-RUnYN-"
      },
      "source": [
        "def validation_stats(network, loader):\r\n",
        "\r\n",
        "    acc = []\r\n",
        "    with torch.no_grad():\r\n",
        "        for x, y in loader:\r\n",
        "          x, y = x.to(device), y.to(device)\r\n",
        "          y_pred = network(x)\r\n",
        "          acc.append(binary_acc(y_pred, y.unsqueeze(1)))\r\n",
        "\r\n",
        "    acc = torch.Tensor(acc)\r\n",
        "    return acc.mean()"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDFkpfbL6WV7",
        "outputId": "3c54c9e3-c2e5-4ddf-dce8-fb69d11966a0"
      },
      "source": [
        "EPOCHS = 20\r\n",
        "\r\n",
        "\r\n",
        "model.train()\r\n",
        "for e in range(1, EPOCHS+1):\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    for X_batch, y_batch in train_loader:\r\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        y_pred = model(X_batch)\r\n",
        "        \r\n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\r\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc.item()\r\n",
        "    val_acc = validation_stats(model, val_loader)\r\n",
        "    print('Epoch {:d}.\\tLoss: {:.5f}\\tAccuracy: {:.3f}% (train) / {:.3f}% (val)'.format(e, 100*epoch_loss/len(train_loader), epoch_acc/len(train_loader), val_acc))\r\n",
        "    scheduler.step()\r\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1.\tLoss: 21.55328\tAccuracy: 91.394% (train) / 91.385% (val)\n",
            "Epoch 2.\tLoss: 20.95680\tAccuracy: 91.601% (train) / 91.490% (val)\n",
            "Epoch 3.\tLoss: 20.88812\tAccuracy: 91.611% (train) / 91.488% (val)\n",
            "Epoch 4.\tLoss: 20.88006\tAccuracy: 91.611% (train) / 91.492% (val)\n",
            "Epoch 5.\tLoss: 20.88742\tAccuracy: 91.605% (train) / 91.479% (val)\n",
            "Epoch 6.\tLoss: 20.88358\tAccuracy: 91.615% (train) / 91.480% (val)\n",
            "Epoch 7.\tLoss: 20.87089\tAccuracy: 91.619% (train) / 91.487% (val)\n",
            "Epoch 8.\tLoss: 20.88478\tAccuracy: 91.610% (train) / 91.480% (val)\n",
            "Epoch 9.\tLoss: 20.88127\tAccuracy: 91.614% (train) / 91.493% (val)\n",
            "Epoch 10.\tLoss: 20.88281\tAccuracy: 91.613% (train) / 91.494% (val)\n",
            "Epoch 11.\tLoss: 20.88625\tAccuracy: 91.614% (train) / 91.481% (val)\n",
            "Epoch 12.\tLoss: 20.87293\tAccuracy: 91.618% (train) / 91.485% (val)\n",
            "Epoch 13.\tLoss: 20.88314\tAccuracy: 91.612% (train) / 91.476% (val)\n",
            "Epoch 14.\tLoss: 20.88719\tAccuracy: 91.613% (train) / 91.477% (val)\n",
            "Epoch 15.\tLoss: 20.88171\tAccuracy: 91.617% (train) / 91.476% (val)\n",
            "Epoch 16.\tLoss: 20.88364\tAccuracy: 91.610% (train) / 91.477% (val)\n",
            "Epoch 17.\tLoss: 20.88441\tAccuracy: 91.613% (train) / 91.485% (val)\n",
            "Epoch 18.\tLoss: 20.87871\tAccuracy: 91.609% (train) / 91.476% (val)\n",
            "Epoch 19.\tLoss: 20.88752\tAccuracy: 91.598% (train) / 91.476% (val)\n",
            "Epoch 20.\tLoss: 20.89026\tAccuracy: 91.598% (train) / 91.487% (val)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0DJgCUr8PDf"
      },
      "source": [
        "X_t = pd.read_csv('gdrive/My Drive/pred_all_models_test.csv').drop(columns = 'Unnamed: 0')\r\n",
        "\r\n",
        "textRep = False\r\n",
        "if textRep:\r\n",
        "  model = 'skipgram' # or cbow\r\n",
        "  X1_0 = np.load('gdrive/My Drive/'+model + '_test_neg_full_u.npy')\r\n",
        "  X1_1 = np.load('gdrive/My Drive/'+model + '_test_pos_full_u.npy')\r\n",
        "  X1 = np.concatenate((X1_0, X1_1))\r\n",
        "  X_t = np.concatenate((X, X1), axis = 1)\r\n",
        "X_t = scaler.fit_transform(X_t)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrZZuyP8_KAo"
      },
      "source": [
        "class testData(Dataset):\r\n",
        "    def __init__(self, X_data):\r\n",
        "        self.X_data = X_data\r\n",
        "        \r\n",
        "    def __getitem__(self, index):\r\n",
        "        return self.X_data[index]\r\n",
        "        \r\n",
        "    def __len__ (self):\r\n",
        "        return len(self.X_data)\r\n",
        "    \r\n",
        "\r\n",
        "test_data = testData(torch.FloatTensor(X_t))\r\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=10000, shuffle=False, drop_last=False)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIqd6qG1_o7G",
        "outputId": "5cb8264d-a355-4aa5-c913-59e22ccb72b7"
      },
      "source": [
        "y_pred_list = []\r\n",
        "model.eval()\r\n",
        "with torch.no_grad():\r\n",
        "    for X_batch in test_loader:\r\n",
        "        X_batch = X_batch.to(device)\r\n",
        "        pred = torch.round(torch.sigmoid(model(X_batch)))\r\n",
        "        y_pred_list.append(pred.cpu().numpy())\r\n",
        "\r\n",
        "predictions = (np.array([a.squeeze().tolist() for a in y_pred_list][0])*2 - 1).astype(int)\r\n",
        "predictions"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1,  1, ..., -1,  1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    }
  ]
}