{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.svm import LinearSVC\n",
    "import helpers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "\n",
    "full = True\n",
    "\n",
    "if full: \n",
    "    pos_filename = 'twitter-datasets/train_pos_full_u.txt'\n",
    "    neg_filename = 'twitter-datasets/train_neg_full_u.txt'\n",
    "else: \n",
    "    pos_filename = 'twitter-datasets/train_pos_u.txt'\n",
    "    neg_filename = 'twitter-datasets/train_neg_u.txt'\n",
    "\n",
    "\n",
    "pos_tweets = helpers.txt_to_list(pos_filename)\n",
    "neg_tweets = helpers.txt_to_list(neg_filename)\n",
    "\n",
    "# Create a labeled dataset \n",
    "all_tweets, y = helpers.merge_shuffle_label(pos_tweets, neg_tweets)\n",
    "\n",
    "# Split into train and validation sets\n",
    "training_fraction = 0.8\n",
    "train, val, y_train, y_val = helpers.split_dataset(training_fraction, all_tweets, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "from preprocessing import process_sentence, to_vec, split_hashtag, remove_repeats, remove_informal_contractions\n",
    "\n",
    "def tk(sent):\n",
    "    tokens = p.tokenize(sent).split()\n",
    "    return tokens\n",
    "\n",
    "def tk2(sent):\n",
    "    tokens = p.tokenize(sent).split()\n",
    "    return pre.process_sentence(tokens, pre.preproc_pipeline)\n",
    "\n",
    "preproc_pipeline = [to_vec(split_hashtag),  \n",
    "                    to_vec(remove_repeats)]\n",
    "\n",
    "def tk3(sent):\n",
    "    tokens = TweetTokenizer().tokenize(sent)\n",
    "    return process_sentence(tokens, preproc_pipeline)\n",
    "\n",
    "#preproc_pipeline = [to_vec(split_hashtag), \n",
    "                  # to_vec(words_to_tags), \n",
    "                  # to_vec(remove_repeats)]\n",
    "\n",
    "def tk4(sent):\n",
    "    tokens = TweetTokenizer().tokenize(sent)\n",
    "    return process_sentence(tokens, preproc_pipeline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1b1197538e98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mTFIDF\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtk3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublinear_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-37c6d05aaaea>\u001b[0m in \u001b[0;36mtk3\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtk3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreproc_pipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_lengthening\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;31m# Shorten problematic sequences of characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0msafe_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHANG_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\1\\1\\1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;31m# Tokenize:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWORD_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build training vectorization \n",
    "TFIDF = True\n",
    "\n",
    "if TFIDF:\n",
    "    vect=TfidfVectorizer(use_idf=True, ngram_range = (1,2), tokenizer = tk3, sublinear_tf = False)\n",
    "    X_train = vect.fit_transform(train)\n",
    "    X_val = vect.transform(val)\n",
    "else:\n",
    "    vect=CountVectorizer(ngram_range = (1,2), tokenizer = tk3)\n",
    "    X_train = vect.fit_transform(train)\n",
    "    X_val = vect.transform(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a few classifiers on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(random_state=0, tol=1e-9, loss = 'squared_hinge', dual = True, C = 0.03)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "helpers.judge_pred(clf, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 strongest bigrams to indicate positive sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>( &gt;</th>\n",
       "      <td>2.001165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no problem</th>\n",
       "      <td>1.044102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( (</th>\n",
       "      <td>1.004770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ways to make me hapy</th>\n",
       "      <td>0.899695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not mising</th>\n",
       "      <td>0.881708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you get major points if</th>\n",
       "      <td>0.867333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.857437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( ^</th>\n",
       "      <td>0.839051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can't wait</th>\n",
       "      <td>0.817740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cant wait</th>\n",
       "      <td>0.789983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>; rt</th>\n",
       "      <td>0.779367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canot wait</th>\n",
       "      <td>0.771748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not sad</th>\n",
       "      <td>0.752395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be jel</th>\n",
       "      <td>0.749756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not woried</th>\n",
       "      <td>0.741655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smart nokia lumi a</th>\n",
       "      <td>0.740236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no probs</th>\n",
       "      <td>0.737980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i find that atractive</th>\n",
       "      <td>0.734761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no prob</th>\n",
       "      <td>0.731977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no ned</th>\n",
       "      <td>0.728914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hary poter chat up lines</th>\n",
       "      <td>0.724574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fels god</th>\n",
       "      <td>0.718787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mising me</th>\n",
       "      <td>0.698913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me luck</th>\n",
       "      <td>0.698627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>why thank</th>\n",
       "      <td>0.698456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapy twet</th>\n",
       "      <td>0.693957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get tired</th>\n",
       "      <td>0.668610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:/ but</th>\n",
       "      <td>0.664825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( &lt;</th>\n",
       "      <td>0.664480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay mad</th>\n",
       "      <td>0.663016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mis me</th>\n",
       "      <td>0.658620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dying my</th>\n",
       "      <td>0.657836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not complaining</th>\n",
       "      <td>0.654784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not scared</th>\n",
       "      <td>0.653908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not bad</th>\n",
       "      <td>0.652912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don't mis</th>\n",
       "      <td>0.649047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not wait</th>\n",
       "      <td>0.647227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt; &gt;</th>\n",
       "      <td>0.646851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnt wait</th>\n",
       "      <td>0.646687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be disapointed</th>\n",
       "      <td>0.646288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want no</th>\n",
       "      <td>0.645704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no wories</th>\n",
       "      <td>0.629334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad anymore</th>\n",
       "      <td>0.628528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no doubt</th>\n",
       "      <td>0.620707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you can get major points if</th>\n",
       "      <td>0.601192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get fucked</th>\n",
       "      <td>0.600552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sory not sory</th>\n",
       "      <td>0.596422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>why yes</th>\n",
       "      <td>0.592836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( :</th>\n",
       "      <td>0.592533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tho (</th>\n",
       "      <td>0.591842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no schol</th>\n",
       "      <td>0.591298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no problems</th>\n",
       "      <td>0.589748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don't hate</th>\n",
       "      <td>0.587302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only ned</th>\n",
       "      <td>0.586321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thoughts during schol</th>\n",
       "      <td>0.585968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ways to begin sex</th>\n",
       "      <td>0.580053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thought during schol</th>\n",
       "      <td>0.579716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get enough</th>\n",
       "      <td>0.579159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( _</th>\n",
       "      <td>0.577854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>now folowing</th>\n",
       "      <td>0.577672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fel fre</th>\n",
       "      <td>0.573956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wah wah</th>\n",
       "      <td>0.570720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( -</th>\n",
       "      <td>0.570188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no bother</th>\n",
       "      <td>0.566772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haven't forgoten</th>\n",
       "      <td>0.564951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never to</th>\n",
       "      <td>0.564854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>won't mis</th>\n",
       "      <td>0.564788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get old</th>\n",
       "      <td>0.564573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hacked by</th>\n",
       "      <td>0.562010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing wrong</th>\n",
       "      <td>0.561068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no stres</th>\n",
       "      <td>0.558948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can't mis</th>\n",
       "      <td>0.558669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mises me</th>\n",
       "      <td>0.553585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not mis</th>\n",
       "      <td>0.550948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 best smels</th>\n",
       "      <td>0.550361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doesn't mean</th>\n",
       "      <td>0.549820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never fail</th>\n",
       "      <td>0.547283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>se why</th>\n",
       "      <td>0.545319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thats why</th>\n",
       "      <td>0.544980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[LAUGH] (</th>\n",
       "      <td>0.544668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just jealous</th>\n",
       "      <td>0.540965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never regret</th>\n",
       "      <td>0.540122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went wel</th>\n",
       "      <td>0.539812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that's why</th>\n",
       "      <td>0.538297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wouldn't mis</th>\n",
       "      <td>0.536020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yay</th>\n",
       "      <td>0.528740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no regrets</th>\n",
       "      <td>0.528128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tomorow (</th>\n",
       "      <td>0.527913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you know i care when</th>\n",
       "      <td>0.527904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you strip</th>\n",
       "      <td>0.526803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>al relationships ned</th>\n",
       "      <td>0.521252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>another round</th>\n",
       "      <td>0.521016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is why</th>\n",
       "      <td>0.519892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don't ned</th>\n",
       "      <td>0.519890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fel great</th>\n",
       "      <td>0.519101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( ~</th>\n",
       "      <td>0.518244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not sory</th>\n",
       "      <td>0.516327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be afraid</th>\n",
       "      <td>0.516192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/20</th>\n",
       "      <td>0.515605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never left</th>\n",
       "      <td>0.513390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             coefficient\n",
       "( >                             2.001165\n",
       "no problem                      1.044102\n",
       "( (                             1.004770\n",
       "ways to make me hapy            0.899695\n",
       "not mising                      0.881708\n",
       "you get major points if         0.867333\n",
       ")                               0.857437\n",
       "( ^                             0.839051\n",
       "can't wait                      0.817740\n",
       "cant wait                       0.789983\n",
       "; rt                            0.779367\n",
       "canot wait                      0.771748\n",
       "not sad                         0.752395\n",
       "be jel                          0.749756\n",
       "not woried                      0.741655\n",
       "smart nokia lumi a              0.740236\n",
       "no probs                        0.737980\n",
       "i find that atractive           0.734761\n",
       "no prob                         0.731977\n",
       "no ned                          0.728914\n",
       "hary poter chat up lines        0.724574\n",
       "fels god                        0.718787\n",
       "mising me                       0.698913\n",
       "me luck                         0.698627\n",
       "why thank                       0.698456\n",
       "hapy twet                       0.693957\n",
       "get tired                       0.668610\n",
       ":/ but                          0.664825\n",
       "( <                             0.664480\n",
       "stay mad                        0.663016\n",
       "mis me                          0.658620\n",
       "dying my                        0.657836\n",
       "not complaining                 0.654784\n",
       "not scared                      0.653908\n",
       "not bad                         0.652912\n",
       "don't mis                       0.649047\n",
       "not wait                        0.647227\n",
       "> >                             0.646851\n",
       "cnt wait                        0.646687\n",
       "be disapointed                  0.646288\n",
       "want no                         0.645704\n",
       "no wories                       0.629334\n",
       "sad anymore                     0.628528\n",
       "no doubt                        0.620707\n",
       "you can get major points if     0.601192\n",
       "get fucked                      0.600552\n",
       "sory not sory                   0.596422\n",
       "why yes                         0.592836\n",
       "( :                             0.592533\n",
       "tho (                           0.591842\n",
       "no schol                        0.591298\n",
       "no problems                     0.589748\n",
       "don't hate                      0.587302\n",
       "only ned                        0.586321\n",
       "thoughts during schol           0.585968\n",
       "ways to begin sex               0.580053\n",
       "thought during schol            0.579716\n",
       "get enough                      0.579159\n",
       "( _                             0.577854\n",
       "now folowing                    0.577672\n",
       "fel fre                         0.573956\n",
       "wah wah                         0.570720\n",
       "( -                             0.570188\n",
       "no bother                       0.566772\n",
       "haven't forgoten                0.564951\n",
       "never to                        0.564854\n",
       "won't mis                       0.564788\n",
       "get old                         0.564573\n",
       "hacked by                       0.562010\n",
       "nothing wrong                   0.561068\n",
       "no stres                        0.558948\n",
       "can't mis                       0.558669\n",
       "mises me                        0.553585\n",
       "not mis                         0.550948\n",
       "5 best smels                    0.550361\n",
       "doesn't mean                    0.549820\n",
       "never fail                      0.547283\n",
       "se why                          0.545319\n",
       "thats why                       0.544980\n",
       "[LAUGH] (                       0.544668\n",
       "just jealous                    0.540965\n",
       "never regret                    0.540122\n",
       "went wel                        0.539812\n",
       "that's why                      0.538297\n",
       "wouldn't mis                    0.536020\n",
       "yay                             0.528740\n",
       "no regrets                      0.528128\n",
       "tomorow (                       0.527913\n",
       "you know i care when            0.527904\n",
       "you strip                       0.526803\n",
       "al relationships ned            0.521252\n",
       "another round                   0.521016\n",
       "is why                          0.519892\n",
       "don't ned                       0.519890\n",
       "fel great                       0.519101\n",
       "( ~                             0.518244\n",
       "not sory                        0.516327\n",
       "be afraid                       0.516192\n",
       "4/20                            0.515605\n",
       "never left                      0.513390"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(clf.coef_.flatten(), index=vect.get_feature_names(), columns=[\"coefficient\"]) \n",
    "n_ = 100\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('{:d} strongest bigrams to indicate positive sentiment'.format(n_))\n",
    "df.sort_values(by=[\"coefficient\"],ascending=False).head(n_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 strongest bigrams to indicate negative sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(8</th>\n",
       "      <td>-1.205499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>=) )</th>\n",
       "      <td>-1.107342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>-1.094412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad twet</th>\n",
       "      <td>-1.043841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not nice</th>\n",
       "      <td>-0.935314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>) &gt;</th>\n",
       "      <td>-0.925660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>its sad that</th>\n",
       "      <td>-0.907030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadest</th>\n",
       "      <td>-0.906200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>-0.887672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>) )</th>\n",
       "      <td>-0.881423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guted</th>\n",
       "      <td>-0.869218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depresing</th>\n",
       "      <td>-0.862251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not hapy</th>\n",
       "      <td>-0.858649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wah</th>\n",
       "      <td>-0.855462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadly</th>\n",
       "      <td>-0.849407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruined</th>\n",
       "      <td>-0.840783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porly</th>\n",
       "      <td>-0.839125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heartbreaking</th>\n",
       "      <td>-0.824954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if women did not exist</th>\n",
       "      <td>-0.822915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the voice uk</th>\n",
       "      <td>-0.811266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hais</th>\n",
       "      <td>-0.810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fml</th>\n",
       "      <td>-0.809339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brats are fun</th>\n",
       "      <td>-0.795607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upseting</th>\n",
       "      <td>-0.791689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funeral</th>\n",
       "      <td>-0.790285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanie</th>\n",
       "      <td>-0.788861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if i could i would</th>\n",
       "      <td>-0.781665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapointing</th>\n",
       "      <td>-0.778028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not loking</th>\n",
       "      <td>-0.774457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapointed</th>\n",
       "      <td>-0.771979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devastated</th>\n",
       "      <td>-0.768702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crying</th>\n",
       "      <td>-0.765381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>' )</th>\n",
       "      <td>-0.762906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfortunately</th>\n",
       "      <td>-0.756123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreading</th>\n",
       "      <td>-0.754074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sometimes i just want</th>\n",
       "      <td>-0.753932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>im mad because</th>\n",
       "      <td>-0.750034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pased away</th>\n",
       "      <td>-0.749837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rip</th>\n",
       "      <td>-0.749060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigh</th>\n",
       "      <td>-0.748476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hurts</th>\n",
       "      <td>-0.747743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>-0.745067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not fair</th>\n",
       "      <td>-0.744003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not ready</th>\n",
       "      <td>-0.742982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hurtful</th>\n",
       "      <td>-0.740289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt; )</th>\n",
       "      <td>-0.739808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rip mrs bieber</th>\n",
       "      <td>-0.737309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if we were together right now</th>\n",
       "      <td>-0.736232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depresed</th>\n",
       "      <td>-0.735622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heartbroken</th>\n",
       "      <td>-0.732870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headache</th>\n",
       "      <td>-0.732451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>por</th>\n",
       "      <td>-0.732124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfair</th>\n",
       "      <td>-0.725481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>its crazy how</th>\n",
       "      <td>-0.721012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w ah</th>\n",
       "      <td>-0.714828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not col</th>\n",
       "      <td>-0.713790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sucks</th>\n",
       "      <td>-0.708701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not wel</th>\n",
       "      <td>-0.705990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cries</th>\n",
       "      <td>-0.702359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the mater</th>\n",
       "      <td>-0.696151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lonely</th>\n",
       "      <td>-0.693898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bumer</th>\n",
       "      <td>-0.691437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad times</th>\n",
       "      <td>-0.688625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not funy</th>\n",
       "      <td>-0.685726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emosh</th>\n",
       "      <td>-0.684179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ugh</th>\n",
       "      <td>-0.684121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mising</th>\n",
       "      <td>-0.682944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one folowed</th>\n",
       "      <td>-0.682349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>( dvd</th>\n",
       "      <td>-0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything as</th>\n",
       "      <td>-0.672489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horible</th>\n",
       "      <td>-0.669682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boho</th>\n",
       "      <td>-0.667524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cry</th>\n",
       "      <td>-0.665902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i exist</th>\n",
       "      <td>-0.661125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weh</th>\n",
       "      <td>-0.660458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the er</th>\n",
       "      <td>-0.658079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cramps</th>\n",
       "      <td>-0.652076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got excited</th>\n",
       "      <td>-0.650212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever folow</th>\n",
       "      <td>-0.648456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not god</th>\n",
       "      <td>-0.647986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where's</th>\n",
       "      <td>-0.644733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i cried when</th>\n",
       "      <td>-0.639658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tears</th>\n",
       "      <td>-0.639585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruining</th>\n",
       "      <td>-0.635747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>- )</th>\n",
       "      <td>-0.635368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walao</th>\n",
       "      <td>-0.635134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upset</th>\n",
       "      <td>-0.634687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what's wrong</th>\n",
       "      <td>-0.634007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x )</th>\n",
       "      <td>-0.633586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sobing</th>\n",
       "      <td>-0.631336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ive noticed that</th>\n",
       "      <td>-0.628546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imis</th>\n",
       "      <td>-0.628379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not fun</th>\n",
       "      <td>-0.626713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sedih</th>\n",
       "      <td>-0.625667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frezing</th>\n",
       "      <td>-0.623161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aching</th>\n",
       "      <td>-0.622266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nobody to</th>\n",
       "      <td>-0.621866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triste</th>\n",
       "      <td>-0.621504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>) &lt;</th>\n",
       "      <td>-0.618543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt; ^</th>\n",
       "      <td>-0.617309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               coefficient\n",
       "(8                               -1.205499\n",
       "=) )                             -1.107342\n",
       "(                                -1.094412\n",
       "sad twet                         -1.043841\n",
       "not nice                         -0.935314\n",
       ") >                              -0.925660\n",
       "its sad that                     -0.907030\n",
       "sadest                           -0.906200\n",
       "sad                              -0.887672\n",
       ") )                              -0.881423\n",
       "guted                            -0.869218\n",
       "depresing                        -0.862251\n",
       "not hapy                         -0.858649\n",
       "wah                              -0.855462\n",
       "sadly                            -0.849407\n",
       "ruined                           -0.840783\n",
       "porly                            -0.839125\n",
       "heartbreaking                    -0.824954\n",
       "if women did not exist           -0.822915\n",
       "the voice uk                     -0.811266\n",
       "hais                             -0.810900\n",
       "fml                              -0.809339\n",
       "brats are fun                    -0.795607\n",
       "upseting                         -0.791689\n",
       "funeral                          -0.790285\n",
       "meanie                           -0.788861\n",
       "if i could i would               -0.781665\n",
       "disapointing                     -0.778028\n",
       "not loking                       -0.774457\n",
       "disapointed                      -0.771979\n",
       "devastated                       -0.768702\n",
       "crying                           -0.765381\n",
       "' )                              -0.762906\n",
       "unfortunately                    -0.756123\n",
       "dreading                         -0.754074\n",
       "sometimes i just want            -0.753932\n",
       "im mad because                   -0.750034\n",
       "pased away                       -0.749837\n",
       "rip                              -0.749060\n",
       "sigh                             -0.748476\n",
       "hurts                            -0.747743\n",
       "died                             -0.745067\n",
       "not fair                         -0.744003\n",
       "not ready                        -0.742982\n",
       "hurtful                          -0.740289\n",
       "< )                              -0.739808\n",
       "rip mrs bieber                   -0.737309\n",
       "if we were together right now    -0.736232\n",
       "depresed                         -0.735622\n",
       "heartbroken                      -0.732870\n",
       "headache                         -0.732451\n",
       "por                              -0.732124\n",
       "unfair                           -0.725481\n",
       "its crazy how                    -0.721012\n",
       "w ah                             -0.714828\n",
       "not col                          -0.713790\n",
       "sucks                            -0.708701\n",
       "not wel                          -0.705990\n",
       "cries                            -0.702359\n",
       "the mater                        -0.696151\n",
       "lonely                           -0.693898\n",
       "bumer                            -0.691437\n",
       "sad times                        -0.688625\n",
       "not funy                         -0.685726\n",
       "emosh                            -0.684179\n",
       "ugh                              -0.684121\n",
       "mising                           -0.682944\n",
       "one folowed                      -0.682349\n",
       "( dvd                            -0.678000\n",
       "anything as                      -0.672489\n",
       "horible                          -0.669682\n",
       "boho                             -0.667524\n",
       "cry                              -0.665902\n",
       "i exist                          -0.661125\n",
       "weh                              -0.660458\n",
       "the er                           -0.658079\n",
       "cramps                           -0.652076\n",
       "got excited                      -0.650212\n",
       "ever folow                       -0.648456\n",
       "not god                          -0.647986\n",
       "where's                          -0.644733\n",
       "i cried when                     -0.639658\n",
       "tears                            -0.639585\n",
       "ruining                          -0.635747\n",
       "- )                              -0.635368\n",
       "walao                            -0.635134\n",
       "upset                            -0.634687\n",
       "what's wrong                     -0.634007\n",
       "x )                              -0.633586\n",
       "sobing                           -0.631336\n",
       "ive noticed that                 -0.628546\n",
       "imis                             -0.628379\n",
       "not fun                          -0.626713\n",
       "sedih                            -0.625667\n",
       "frezing                          -0.623161\n",
       "aching                           -0.622266\n",
       "nobody to                        -0.621866\n",
       "triste                           -0.621504\n",
       ") <                              -0.618543\n",
       "< ^                              -0.617309"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('{:d} strongest bigrams to indicate negative sentiment'.format(n_))\n",
    "df.sort_values(by=[\"coefficient\"],ascending=False).tail(n_).sort_values(by=[\"coefficient\"],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most confident correct predictions of positive tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>today's gonna be a good day . 4/20 4/20 4/20 4/20 4/20 4/20 4/20 4/20 4/20 4/20 happy four tweenyyy stoners\\n</th>\n",
       "      <td>6.540538</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yay yay yay yay ) ) chris said he'd sing me a song tomorrow if i go ) woohooo whooohoo but its a surprise which song : b\\n</th>\n",
       "      <td>5.781296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; awww ur welcome i can tell ur so excited i would be too ! ! ) i am happy for you girl ! ! ) ) &lt;3\\n</th>\n",
       "      <td>4.830495</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; goodmorning and you have a nice day as well ! thank you and god bless you ! keep that smile on your face ) proverbs 15:13\\n</th>\n",
       "      <td>4.736957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; &lt;user&gt; please greet my cousin ) raya ) we are listening right now ) and please play the song young wild and free ) ...\\n</th>\n",
       "      <td>4.674764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let's just enjoy life .. smile and don't let anyone ruin your day . ) ) ) smile my dear ) have a great day ! ! imissyou &lt;3\\n</th>\n",
       "      <td>4.652457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; please greet my cousin ) raya ) we are listening right now ) and please play the song young wild and free )\\n</th>\n",
       "      <td>4.588453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; wants that grace , grace , grace , grace , grace , grace , grace , grace , grace , grace , grace , grace , grace , grace , grace !\\n</th>\n",
       "      <td>4.570932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makasii tata .. ) rt &lt;user&gt; happy birthdayy &lt;user&gt; ! hope your day is filled with happiness . god bless ! )\\n</th>\n",
       "      <td>4.528263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; finally ! ! ! best scene ever ! i got butterflies in my stomach ! thank you , thank you , thank youu ! ) elena and damon is trending :d\\n</th>\n",
       "      <td>4.430420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    coefficient  label\n",
       "tweet                                                                 \n",
       "today's gonna be a good day . 4/20 4/20 4/20 4/...     6.540538      1\n",
       "yay yay yay yay ) ) chris said he'd sing me a s...     5.781296      1\n",
       "<user> awww ur welcome i can tell ur so excited...     4.830495      1\n",
       "<user> goodmorning and you have a nice day as w...     4.736957      1\n",
       "<user> <user> please greet my cousin ) raya ) w...     4.674764      1\n",
       "let's just enjoy life .. smile and don't let an...     4.652457      1\n",
       "<user> please greet my cousin ) raya ) we are l...     4.588453      1\n",
       "<user> wants that grace , grace , grace , grace...     4.570932      1\n",
       "makasii tata .. ) rt <user> happy birthdayy <us...     4.528263      1\n",
       "<user> finally ! ! ! best scene ever ! i got bu...     4.430420      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dict(zip(['tweet', 'coefficient', 'label'], [val, clf.decision_function(X_val), y_val])))\n",
    "df.set_index('tweet', inplace = True)\n",
    "n_ = 10\n",
    "print('{:d} most confident correct predictions of positive tweets'.format(n_))\n",
    "df.query('label == 1').sort_values(by= 'coefficient', ascending = False).head(n_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most confident incorrect predictions of positive tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>( subject homonyms ( word dew ( assignment use the word in a sentence ( student i wear a dew rag . - - - really ? ! ? !\\n</th>\n",
       "      <td>-3.945326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i just wish i could hear that you got beat tf up so bad that your in the hospital , bloody , broken bones , all that hatee this nigga .\\n</th>\n",
       "      <td>-3.519090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my timeline is full of sick people that include me .. ader sakit kepala , sakit mata , sakit nak demam .. eh eh sakit hati pon ader ..\\n</th>\n",
       "      <td>-3.351556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride aside , i miss him . i miss what we were &amp; was hoping for a friendship . but it didnt work like that . ( miss that baton too\\n</th>\n",
       "      <td>-3.290970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crying crying crying crying crying &lt;user&gt; &lt;3 #lifeonmurs\\n</th>\n",
       "      <td>-3.263616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i woke up crying ... also i was late for school and forgot my glasses at home so i had to cross this mean old mans garden but he let me go\\n</th>\n",
       "      <td>-3.136128</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; i hate science ! my friend and i were just talking about how we hate it . it's so hard and confusing and ughh i hate it !\\n</th>\n",
       "      <td>-3.133745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; no i'm not gay ! the spider was huge and just gross . i cried because the show was sad . sorry im an emotional guy #hopoff\\n</th>\n",
       "      <td>-3.073798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my tan is fading fast . this maketh me sad ... and i don't like feeling sad . some 1 send me some jokes\\n</th>\n",
       "      <td>-3.022196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; how sad you never wish my birthday yesterday .. mind to follow me back it just a dreaming to get wish from you . #120\\n</th>\n",
       "      <td>-2.924709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    coefficient  label\n",
       "tweet                                                                 \n",
       "( subject homonyms ( word dew ( assignment use ...    -3.945326      1\n",
       "i just wish i could hear that you got beat tf u...    -3.519090      1\n",
       "my timeline is full of sick people that include...    -3.351556      1\n",
       "pride aside , i miss him . i miss what we were ...    -3.290970      1\n",
       "crying crying crying crying crying <user> <3 #l...    -3.263616      1\n",
       "i woke up crying ... also i was late for school...    -3.136128      1\n",
       "<user> i hate science ! my friend and i were ju...    -3.133745      1\n",
       "<user> no i'm not gay ! the spider was huge and...    -3.073798      1\n",
       "my tan is fading fast . this maketh me sad ... ...    -3.022196      1\n",
       "<user> how sad you never wish my birthday yeste...    -2.924709      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('{:d} most confident incorrect predictions of positive tweets'.format(n_))\n",
    "df.query('label == 1').sort_values(by= 'coefficient', ascending = True).head(n_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most confident correct predictions of negative tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>it's cold in here ( ( and i'm all alone ( ( i'm cold : ( ( ( guise i can't breathe ( i hate rummm !\\n</th>\n",
       "      <td>-7.723273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seriously huhu this is so depressing ( ( ( still watching ollg vids ( ( ( wooo ( ( ( may 10 huhuhuhu pls come back\\n</th>\n",
       "      <td>-6.298245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smart debt solution : will credit counseling help improve ... : bank ( 4 bankruptcy ( 4 budget ( 23 business ( 1 ... &lt;url&gt;\\n</th>\n",
       "      <td>-5.966434</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lonely i'm so lonely i have nobody to call my own i'm so lonely , i'm mr . lonely i have nobody to call my own i'm so lonely\\n</th>\n",
       "      <td>-5.762936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no no no ! ! why did mike haveto die ? ? ? i've never cried so much in all my life ! dh wont be the same without him ! ! the funeral was soo sad\\n</th>\n",
       "      <td>-5.700586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hikaru no go , vol . 14 ( hikaru no go ( graphic novels ) ) ( paperback r to l ( japanese style ) after stumbling across ... &lt;url&gt;\\n</th>\n",
       "      <td>-5.674365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; yep apparently ( ( day ruined ( ( do u have any suggestion to give me ? i can't send dms anymore , don't know what to do ( (\\n</th>\n",
       "      <td>-5.606302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>why must sleep torture me with nightmares ugh this day sucks never been so heart broke in my life i need someone here please help me\\n</th>\n",
       "      <td>-5.605954</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tour depression tour depression tour depression ! can't believe the 4d tour is over in london whyy ? ! can't it just last forever ?\\n</th>\n",
       "      <td>-5.539437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rose to miss rest of season with torn acl ( yahoo ! sports chicago ( ap ) bulls star derrick rose will miss the re ... &lt;url&gt;\\n</th>\n",
       "      <td>-5.506700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    coefficient  label\n",
       "tweet                                                                 \n",
       "it's cold in here ( ( and i'm all alone ( ( i'm...    -7.723273      0\n",
       "seriously huhu this is so depressing ( ( ( stil...    -6.298245      0\n",
       "smart debt solution : will credit counseling he...    -5.966434      0\n",
       "lonely i'm so lonely i have nobody to call my o...    -5.762936      0\n",
       "no no no ! ! why did mike haveto die ? ? ? i've...    -5.700586      0\n",
       "hikaru no go , vol . 14 ( hikaru no go ( graphi...    -5.674365      0\n",
       "<user> yep apparently ( ( day ruined ( ( do u h...    -5.606302      0\n",
       "why must sleep torture me with nightmares ugh t...    -5.605954      0\n",
       "tour depression tour depression tour depression...    -5.539437      0\n",
       "rose to miss rest of season with torn acl ( yah...    -5.506700      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('{:d} most confident correct predictions of negative tweets'.format(n_))\n",
    "df.query('label == 0').sort_values(by= 'coefficient', ascending = True).head(n_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most confident incorrect predictions of negative tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>my younger sister , has got her birthday today ! so we ve a big party ! ! :/ ) tomorrow for her :/ / ) ! ! ! happy . birthday . :/ ) &lt;3\\n</th>\n",
       "      <td>4.797551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; 1 ) you're humble 2 ) cool hair 3 ) realistic 4 ) kickass taste in music 5 ) not a spammer ... i don't have enough characters\\n</th>\n",
       "      <td>3.264121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; &lt;user&gt; lmao mhmmm , 2strokez - - - gotta say it fast ---&gt; that all i got , that all i got . ! ) .. lol na ty will get the job done . !\\n</th>\n",
       "      <td>3.252798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; &lt;user&gt; thank you both for your words , i appreciate it greatly ! thank you ! ! )\\n</th>\n",
       "      <td>3.136788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me singing * if that's ya hoe , thats my hoe too * ) smiley : ayeee ! that's that shit\\n</th>\n",
       "      <td>3.020325</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boy rubbing on leg ) so wats ur horoscope sign ? girl : \" stop \" boy moves hand ) so wat is it ? girl : nigga the red sign that says stop cuz u need 2\\n</th>\n",
       "      <td>2.983551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;user&gt; hahahahah ! christie not'a happy bunny with twitter uhh'ohh hahah ! im seeing you tonightt , yay yaay yaaay ! ! ! #loveyou\\n</th>\n",
       "      <td>2.787386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol \" &lt;user&gt; .. ) i tweet a fuck of a lot sometimes . so while it's great to meet you on twitter , you will unfollow me . happy , haters ? \"\\n</th>\n",
       "      <td>2.783875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you : hi person :d o i know u you : no but i know u person , wth ( calls police ) you get arrested ) you get out of jail ) person restraning order )\\n</th>\n",
       "      <td>2.687425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hey &lt;user&gt; thanks for the ff . happy friday )\\n</th>\n",
       "      <td>2.673956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    coefficient  label\n",
       "tweet                                                                 \n",
       "my younger sister , has got her birthday today ...     4.797551      0\n",
       "<user> 1 ) you're humble 2 ) cool hair 3 ) real...     3.264121      0\n",
       "<user> <user> lmao mhmmm , 2strokez - - - gotta...     3.252798      0\n",
       "<user> <user> thank you both for your words , i...     3.136788      0\n",
       "me singing * if that's ya hoe , thats my hoe to...     3.020325      0\n",
       "boy rubbing on leg ) so wats ur horoscope sign ...     2.983551      0\n",
       "<user> hahahahah ! christie not'a happy bunny w...     2.787386      0\n",
       "lol \" <user> .. ) i tweet a fuck of a lot somet...     2.783875      0\n",
       "you : hi person :d o i know u you : no but i kn...     2.687425      0\n",
       "hey <user> thanks for the ff . happy friday )\\n        2.673956      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('{:d} most confident incorrect predictions of negative tweets'.format(n_))\n",
    "df.query('label == 0').sort_values(by= 'coefficient', ascending = False).head(n_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 86.35% / validation set: 81.52%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "mdb = MultinomialNB()\n",
    "mdb.fit(X_train, y_train)\n",
    "\n",
    "helpers.judge_pred(mdb, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 80.62% / validation set: 75.64%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "helpers.judge_pred(clf, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 80.62% / validation set: 75.64%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "helpers.judge_pred(clf, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 85.34% / validation set: 84.44%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf =  linear_model.SGDClassifier(loss = 'hinge', max_iter=int(1e7), tol=1e-5, verbose = False)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "helpers.judge_pred(clf, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 96.80% / validation set: 81.79%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf =  linear_model.Perceptron()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "helpers.judge_pred(clf, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 397. MiB for an array with shape (52029241,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c8902cf2cb1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             )\n\u001b[1;32m--> 303\u001b[1;33m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[0;32m    304\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m         array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    576\u001b[0m                                       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                                       \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[1;31m# create new with correct sparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mspmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             \u001b[0mchanged_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[1;31m# Forward the copy kwarg, if it's accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\scipy\\sparse\\csr.py\u001b[0m in \u001b[0;36mtocsc\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         csr_tocsc(self.shape[0], self.shape[1],\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 397. MiB for an array with shape (52029241,) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "helpers.judge_pred(clf, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Training set accuracy: 84.93%\n"
     ]
    }
   ],
   "source": [
    "# Compute tf-idf on full training set      \n",
    "vect=TfidfVectorizer(use_idf=True, ngram_range = (1,2), tokenizer = tk3, sublinear_tf = False)\n",
    "print(\"1\")\n",
    "X_train_final = vect.fit_transform(all_tweets)\n",
    "print(\"2\")\n",
    "# Check training accuracy\n",
    "clf = LinearSVC(random_state=0, tol=1e-9, loss = 'squared_hinge', dual = True, C = 0.01)\n",
    "clf.fit(X_train_final, y)\n",
    "\n",
    "train_acc = (clf.predict(X_train_final) == y).mean()\n",
    "print('Training set accuracy: {:.2f}%'.format(100*train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'stage2data/pred_test_tfidf.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2ef7171a7232>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Decision_function'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stage2data/pred_test_tfidf.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3165\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3166\u001b[0m         )\n\u001b[1;32m-> 3167\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'stage2data/pred_test_tfidf.csv'"
     ]
    }
   ],
   "source": [
    "# Prepare test set\n",
    "test_tweets = []\n",
    "with open('twitter-datasets/test_data.txt', encoding = 'utf-8') as f: \n",
    "    for line in f:\n",
    "        sp = line.split(',')\n",
    "        index = sp[0]\n",
    "        test_tweets.append(','.join(sp[1:]))\n",
    "X_test = vect.transform(test_tweets)\n",
    "df = pd.DataFrame(clf.decision_function(X_test), columns = ['Decision_function'])\n",
    "df.to_csv('stage2data/pred_test_tfidf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test set\n",
    "tweets = []\n",
    "with open('stage2data/train_neg_full_u.txt', encoding = 'utf-8') as f: # stage2data/train_all_full_u.txt\n",
    "    for line in f:\n",
    "        tweets.append(line)\n",
    "X = vect.transform(tweets)\n",
    "df = pd.DataFrame(clf.decision_function(X), columns = ['Decision_function'])\n",
    "df.to_csv('stage2data/pred_train_neg_u_tfidf.csv')\n",
    "np.save('stage2data/tfidf_train_neg_u.npy', X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
