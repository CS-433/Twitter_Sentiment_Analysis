{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **stage2** (tweet-aware stacking) - Demo\n",
    "This notebook shows how we combined results from different models with stacking in a model that is also aware of the input tweet, using pytorch. It was run in Google colab.\n",
    "\n",
    "\n",
    "## Sources\n",
    "- pytorch (https://pytorch.org/)\n",
    "- Google colab (https://colab.research.google.com/)\n",
    "\n",
    "## Reproducibility\n",
    "Files to run this notebook have not been made available.Itw as used to create Submission (**#109820**) on AIcrowd:\n",
    "\n",
    "| Accuracy | F1 |\n",
    "|:---:|:---:|\n",
    "| 89.9% | 90.0% |\n",
    "\n",
    "### Loading results from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q228h_LE2X4w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76JbgNqUhAU9",
    "outputId": "22e571e2-1e52-48b0-cd00-4190d916f382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv('gdrive/My Drive/pred_all_models_train.csv').drop(columns = 'Unnamed: 0')\n",
    "\n",
    "X = df.iloc[:, 1:].values\n",
    "\n",
    "textRep = True\n",
    "if textRep:\n",
    "  embedding = 'cbow' # or cbow\n",
    "  X1_0 = np.load('gdrive/My Drive/'+embedding + '_train_neg_full_u.npy')\n",
    "  X1_1 = np.load('gdrive/My Drive/'+embedding + '_train_pos_full_u.npy')\n",
    "  X1 = np.concatenate((X1_0, X1_1))\n",
    "  X = np.concatenate((X, X1), axis = 1)\n",
    "\n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNTYiARb5Amf"
   },
   "outputs": [],
   "source": [
    "class trainData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "dim_pred = df.shape[1] - 1\n",
    "xdim = X.shape[1]\n",
    "del df, X, y\n",
    "train_data = trainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "val_data = trainData(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "del X_train, X_val, y_train, y_val\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, drop_last = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WrffIJY6F-M"
   },
   "outputs": [],
   "source": [
    "class clf3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(clf3, self).__init__()\n",
    "        self.dense = nn.Linear(xdim, xdim) \n",
    "        self.output_layer = nn.Linear(xdim, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.drop(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class clf3_sep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(clf3_sep, self).__init__()\n",
    "        self.dense = nn.Linear(xdim-dim_pred, 4) \n",
    "        self.output_layer = nn.Linear(4 + dim_pred, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        important_features = inputs[:,:dim_pred]\n",
    "        x = inputs[:,dim_pred:]\n",
    "        x = self.drop(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        important_features = important_features\n",
    "\n",
    "        \n",
    "        x_all = third_tensor = torch.cat((x, important_features), axis = 1)\n",
    "\n",
    "        x_all = self.activation(x_all)\n",
    "        x_all = self.drop(x_all)\n",
    "        x_all = self.output_layer(x_all)\n",
    "        \n",
    "        return x_all\n",
    "\n",
    "\n",
    "class clf3_sep_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(clf3_sep_2, self).__init__()\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        opt_gain = torch.nn.init.calculate_gain('tanh')\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.dense = nn.Linear(xdim-dim_pred, 4) \n",
    "        torch.nn.init.xavier_uniform_(self.dense.weight, gain = opt_gain)\n",
    "\n",
    "        self.dense_feat = nn.Linear(dim_pred, dim_pred) \n",
    "        torch.nn.init.xavier_uniform_(self.dense_feat.weight, gain = opt_gain)\n",
    "\n",
    "        self.output_layer = nn.Linear(4 + dim_pred, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.output_layer.weight, gain = 1)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        important_features = inputs[:,:dim_pred]\n",
    "        x = inputs[:,dim_pred:]\n",
    "        x = self.drop(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        important_features = self.dense_feat(important_features)\n",
    "\n",
    "        \n",
    "        x_all = third_tensor = torch.cat((x, important_features), axis = 1)\n",
    "\n",
    "        x_all = self.activation(x_all)\n",
    "        x_all = self.drop(x_all)\n",
    "        x_all = self.output_layer(x_all)\n",
    "        \n",
    "        return x_all\n",
    "\n",
    "class stage2clf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(stage2clf, self).__init__()\n",
    "        self.hidden_layer_1 = nn.Linear(xdim, 64) \n",
    "        self.hidden_layer_2 = nn.Linear(64, 64)\n",
    "        self.output_layer = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.batch_norm = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.relu(self.hidden_layer_1(x))\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(self.hidden_layer_2(x))\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class stage2clf2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(stage2clf2, self).__init__()\n",
    "        self.hidden_layer_1 = nn.Linear(xdim, 128) \n",
    "        self.hidden_layer_2 = nn.Linear(128, 64)\n",
    "        self.output_layer = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.relu(self.hidden_layer_1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(self.hidden_layer_2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class stage2clf2_init(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(stage2clf2_init, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        opt_gain = torch.nn.init.calculate_gain('relu')\n",
    "\n",
    "        self.hidden_layer_1 = nn.Linear(xdim, 128) \n",
    "        torch.nn.init.xavier_uniform_(self.hidden_layer_1.weight, gain = opt_gain)\n",
    "\n",
    "        self.hidden_layer_2 = nn.Linear(128, 64)\n",
    "        torch.nn.init.xavier_uniform_(self.hidden_layer_2.weight, gain = opt_gain)\n",
    "\n",
    "\n",
    "        self.output_layer = nn.Linear(64, 1) \n",
    "        torch.nn.init.xavier_uniform_(self.output_layer.weight, gain = 1)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.activation(self.hidden_layer_1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.activation(self.hidden_layer_2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2ALqTOf6Ngo"
   },
   "outputs": [],
   "source": [
    "model = stage2clf2_init()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSTZmHqo6UVv"
   },
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIjnd-RUnYN-"
   },
   "outputs": [],
   "source": [
    "def validation_stats(network, loader):\n",
    "\n",
    "    acc = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "          x, y = x.to(device), y.to(device)\n",
    "          y_pred = network(x)\n",
    "          acc.append(binary_acc(y_pred, y.unsqueeze(1)))\n",
    "\n",
    "    acc = torch.Tensor(acc)\n",
    "    return acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDFkpfbL6WV7",
    "outputId": "167dd954-8fd0-427a-b355-035dc48f328c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.\tLoss: 21.62770\tAccuracy: 91.370% (train) / 91.619% (val)\n",
      "Epoch 2.\tLoss: 20.90907\tAccuracy: 91.639% (train) / 91.607% (val)\n",
      "Epoch 3.\tLoss: 20.79532\tAccuracy: 91.673% (train) / 91.642% (val)\n",
      "Epoch 4.\tLoss: 20.73058\tAccuracy: 91.688% (train) / 91.701% (val)\n",
      "Epoch 5.\tLoss: 20.69027\tAccuracy: 91.693% (train) / 91.677% (val)\n",
      "Epoch 6.\tLoss: 20.64886\tAccuracy: 91.712% (train) / 91.691% (val)\n",
      "Epoch 7.\tLoss: 20.60532\tAccuracy: 91.732% (train) / 91.664% (val)\n",
      "Epoch 8.\tLoss: 20.58396\tAccuracy: 91.731% (train) / 91.700% (val)\n",
      "Epoch 9.\tLoss: 20.55064\tAccuracy: 91.750% (train) / 91.702% (val)\n",
      "Epoch 10.\tLoss: 20.52824\tAccuracy: 91.756% (train) / 91.707% (val)\n",
      "Epoch 11.\tLoss: 20.49924\tAccuracy: 91.761% (train) / 91.701% (val)\n",
      "Epoch 12.\tLoss: 20.47783\tAccuracy: 91.781% (train) / 91.735% (val)\n",
      "Epoch 13.\tLoss: 20.44536\tAccuracy: 91.779% (train) / 91.739% (val)\n",
      "Epoch 14.\tLoss: 20.42752\tAccuracy: 91.790% (train) / 91.745% (val)\n",
      "Epoch 15.\tLoss: 20.40386\tAccuracy: 91.803% (train) / 91.741% (val)\n",
      "Epoch 16.\tLoss: 20.37054\tAccuracy: 91.805% (train) / 91.721% (val)\n",
      "Epoch 17.\tLoss: 20.35355\tAccuracy: 91.823% (train) / 91.715% (val)\n",
      "Epoch 18.\tLoss: 20.32378\tAccuracy: 91.833% (train) / 91.759% (val)\n",
      "Epoch 19.\tLoss: 20.31040\tAccuracy: 91.839% (train) / 91.771% (val)\n",
      "Epoch 20.\tLoss: 20.28813\tAccuracy: 91.839% (train) / 91.748% (val)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    val_acc = validation_stats(model, val_loader)\n",
    "    print('Epoch {:d}.\\tLoss: {:.5f}\\tAccuracy: {:.3f}% (train) / {:.3f}% (val)'.format(e, 100*epoch_loss/len(train_loader), epoch_acc/len(train_loader), val_acc))\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOLI7qYB6ODY",
    "outputId": "fa953475-b89b-44ff-d4c6-8576b2fcd5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0DJgCUr8PDf"
   },
   "outputs": [],
   "source": [
    "X_t = pd.read_csv('gdrive/My Drive/pred_all_models_test.csv').drop(columns = 'Unnamed: 0')\n",
    "\n",
    "if textRep:\n",
    "  X1 = np.load('gdrive/My Drive/'+ embedding + '_test.npy')\n",
    "  X_t = np.concatenate((X_t, X1), axis = 1)\n",
    "X_t = scaler.transform(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrZZuyP8_KAo"
   },
   "outputs": [],
   "source": [
    "class testData(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_t))\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=10, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIqd6qG1_o7G"
   },
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        pred = torch.round(torch.sigmoid(model(X_batch)))\n",
    "        y_pred_list.append(pred.cpu().numpy())\n",
    "\n",
    "y_pred = [a.squeeze().tolist() for a in y_pred_list]\n",
    "predictions = []\n",
    "for row in y_pred:\n",
    "  predictions += row\n",
    "predictions = np.array(predictions)\n",
    "preds = pd.DataFrame((2*predictions-1).astype(int), columns = ['Prediction'], index = np.arange(1, len(predictions)+1))\n",
    "preds.index.names = ['Id']\n",
    "preds.to_csv('stage2_nn_init.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVvTJ1zZwlrr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stage2nn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
