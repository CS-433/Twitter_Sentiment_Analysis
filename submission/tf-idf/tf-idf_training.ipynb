{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term-frequency inverse documentation frequency\n",
    "## Classification with Linear SVC\n",
    "\n",
    "## Reproducibility\n",
    "After running this notebook, you will obtain the model used for Submission **#109900** on AIcrowd\n",
    "\n",
    "| Accuracy | F1 |\n",
    "|:---:|:---:|\n",
    "| 86.2% | 86.5% |\n",
    "\n",
    "### Import modules and download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "100% [............................................................................] 817297 / 817297"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import helpers\n",
    "from preprocessing import process_sentence, to_vec, split_hashtag, remove_repeats, remove_informal_contractions\n",
    "import pickle\n",
    "import os \n",
    "import wget\n",
    "root = 'data/'\n",
    "\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "seed = 0\n",
    "    \n",
    "# Download negative full\n",
    "neg_url = 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly8xZHJ2Lm1zL3QvcyFBclREZ3U5ejdJT1ZqcDQ0eDZMdDI5WXBlVXYyZGc_ZT1ZZDJn/root/content'\n",
    "neg_filename = root + 'train_neg_full_u.txt'\n",
    "wget.download(neg_url, neg_filename)\n",
    "neg_tweets = helpers.txt_to_list(neg_filename)\n",
    "\n",
    "# Download positive full\n",
    "pos_url = 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly8xZHJ2Lm1zL3QvcyFBclREZ3U5ejdJT1ZqcDQzcTc3QmNPbUdIWHQ3TXc_ZT01ejdG/root/content'\n",
    "pos_filename = root + 'train_pos_full_u.txt'\n",
    "wget.download(pos_url, pos_filename)\n",
    "pos_tweets = helpers.txt_to_list(pos_filename)\n",
    "\n",
    "# Create a labeled dataset \n",
    "all_tweets, y = helpers.merge_shuffle_label(pos_tweets, neg_tweets, seed = 0)\n",
    "\n",
    "# Prepare test set\n",
    "test_url = 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly8xZHJ2Lm1zL3QvcyFBclREZ3U5ejdJT1ZqcDR5Q3hoWXM4T2FJd1JLenc_ZT1hSXh0/root/content'\n",
    "test_filename = root + 'test.txt'\n",
    "wget.download(test_url, test_filename)\n",
    "\n",
    "test_tweets = []\n",
    "with open(test_filename, encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        sp = line.split(',')\n",
    "        index = sp[0]\n",
    "        test_tweets.append(','.join(sp[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up TF-IDF setting and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipeline = [to_vec(split_hashtag),  \n",
    "                    to_vec(remove_repeats)]\n",
    "def tk(sent):\n",
    "    \"\"\" Tokenize a tweet.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        sent: string\n",
    "            a tweet\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: list of strings\n",
    "            a tokenized version of the string\n",
    "    \"\"\"\n",
    "    tokens = TweetTokenizer().tokenize(sent)\n",
    "    tokens = process_sentence(tokens, preproc_pipeline)\n",
    "    return tokens\n",
    "\n",
    "vect = TfidfVectorizer(use_idf=True, ngram_range = (1,2), tokenizer = tk, sublinear_tf = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute tf-idf on full training set and transform test set   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = vect.fit_transform(all_tweets)\n",
    "X_test = vect.transform(test_tweets)\n",
    "\n",
    "# Save the fitted vectorizer\n",
    "save_filename = root + 'tf-idf_fitted_vectorizer.pkl'\n",
    "with open(save_filename, 'wb') as file:\n",
    "    pickle.dump(vect, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0, tol=1e-9, loss = 'squared_hinge', dual = True, C = 0.03)\n",
    "clf.fit(X_train_final, y)\n",
    "\n",
    "# Save the trained classifier\n",
    "save_filename = root + 'tf-idf_trained_linearSVC.pkl'\n",
    "with open(save_filename, 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 86.72%\n"
     ]
    }
   ],
   "source": [
    "train_acc = (clf.predict(X_train_final) == y).mean()\n",
    "print('Training set accuracy: {:.2f}%'.format(100*train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filename = 'submission_tfidf.csv'\n",
    "predictions = clf.predict(X_test)\n",
    "helpers.save_pred(save_filename, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
